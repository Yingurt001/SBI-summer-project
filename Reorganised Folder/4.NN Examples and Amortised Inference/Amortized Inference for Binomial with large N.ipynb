{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8adf8ec-b58e-4f61-9f44-0f54c1960a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.stats import beta, norm, ks_2samp, wasserstein_distance\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "\n",
    "def generate_theta_y_n_large(batch_size=256, n_min=50, n_max=100_000, log_uniform=True):\n",
    "    if log_uniform:\n",
    "        log_n = np.random.uniform(np.log(n_min), np.log(n_max), size=(batch_size, 1))\n",
    "        n = np.round(np.exp(log_n)).astype(np.float32)\n",
    "    else:\n",
    "        n = np.random.randint(n_min, n_max + 1, size=(batch_size, 1)).astype(np.float32)\n",
    "    theta = np.random.uniform(0, 1, size=(batch_size, 1)).astype(np.float32)\n",
    "    y = np.random.binomial(n=n.astype(int), p=theta).astype(np.float32)\n",
    "    return y, n, theta\n",
    "\n",
    "def gmm_log_likelihood(theta, logits, mus, log_sigmas):\n",
    "    log_sigmas = torch.clamp(log_sigmas, min=-5, max=3)\n",
    "    sigmas = torch.exp(log_sigmas)\n",
    "    pi = torch.softmax(logits, dim=1)\n",
    "    th = theta.unsqueeze(1).expand(-1, mus.size(1))\n",
    "    logp = -0.5 * ((th - mus) / sigmas) ** 2 - log_sigmas - 0.5 * np.log(2 * np.pi)\n",
    "    wlogp = logp + torch.log(pi + 1e-10)\n",
    "    return -torch.mean(torch.logsumexp(wlogp, dim=1))\n",
    "\n",
    "class GMMInferenceNet(nn.Module):\n",
    "    def __init__(self, num_components=3, hidden_dims=(512, 512, 256, 128)):\n",
    "        super().__init__()\n",
    "        self.K = num_components\n",
    "        layers = []\n",
    "        d = 2\n",
    "        for h in hidden_dims:\n",
    "            layers += [nn.Linear(d, h), nn.ReLU()]\n",
    "            d = h\n",
    "        layers.append(nn.Linear(d, 3 * self.K))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out[:, :self.K], out[:, self.K:2*self.K], out[:, 2*self.K:]\n",
    "\n",
    "def train_gmm_network(model, optimizer, num_epochs=200, batch_size=256, n_min=50, n_max=100_000, steps_per_epoch=100):\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for _ in range(steps_per_epoch):\n",
    "            y_np, n_np, theta_np = generate_theta_y_n_large(batch_size, n_min, n_max)\n",
    "            x = torch.cat([\n",
    "                torch.from_numpy(y_np / n_np),\n",
    "                torch.from_numpy(np.log(n_np))\n",
    "            ], dim=1).float()\n",
    "            theta = torch.from_numpy(theta_np).squeeze(1).float()\n",
    "            logits, mus, logs = model(x)\n",
    "            pi = torch.softmax(logits, dim=1)\n",
    "            nll = gmm_log_likelihood(theta, logits, mus, logs)\n",
    "            entropy = -torch.sum(pi * torch.log(pi + 1e-8), dim=1).mean()\n",
    "            loss = nll - 0.01 * entropy\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"[GMM] Epoch {epoch + 1}/{num_epochs} | Loss: {total_loss / steps_per_epoch:.4f}\")\n",
    "\n",
    "def evaluate_gmm_fixed_n(model, fixed_n, y_values, theta_grid, num_components=3):\n",
    "    plt.figure(figsize=(10, 3.5 * len(y_values)))\n",
    "    for i, y in enumerate(y_values):\n",
    "        alpha, beta_param = y + 1, fixed_n - y + 1\n",
    "        true_pdf = beta.pdf(theta_grid, alpha, beta_param)\n",
    "        x = torch.tensor([[y / fixed_n, np.log(fixed_n)]], dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            logits, mus, logs = model(x)\n",
    "        pi = torch.softmax(logits, dim=1).numpy().flatten()\n",
    "        mus = mus.numpy().flatten()\n",
    "        sig = np.exp(logs.numpy()).flatten()\n",
    "        gmm_pdf = sum(pi[k] * norm.pdf(theta_grid, mus[k], sig[k]) for k in range(num_components))\n",
    "        ts = beta.rvs(alpha, beta_param, size=2000)\n",
    "        ss = np.clip(np.hstack([\n",
    "            np.random.normal(mus[k], sig[k], int(round(pi[k] * 2000)))\n",
    "            for k in range(num_components)\n",
    "        ]), 0, 1)\n",
    "        ks, p = ks_2samp(ts, ss)\n",
    "        wd = wasserstein_distance(ts, ss)\n",
    "        ax = plt.subplot(len(y_values), 1, i + 1)\n",
    "        ax.plot(theta_grid, true_pdf, 'b--')\n",
    "        ax.plot(theta_grid, gmm_pdf, 'r-')\n",
    "        ax.set_title(f\"GMM Fixed n={fixed_n}, y={y} | KS={ks:.3f}, p={p:.3f}, W={wd:.3f}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"plots/gmm_fixedN_{fixed_n}.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_gmm_unseen_n(model, test_cases, theta_grid, num_components=3):\n",
    "    plt.figure(figsize=(10, 3.5 * len(test_cases)))\n",
    "    for i, (n, y) in enumerate(test_cases):\n",
    "        alpha, beta_param = y + 1, n - y + 1\n",
    "        true_pdf = beta.pdf(theta_grid, alpha, beta_param)\n",
    "        x = torch.tensor([[y / n, np.log(n)]], dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            logits, mus, logs = model(x)\n",
    "        pi = torch.softmax(logits, dim=1).numpy().flatten()\n",
    "        mus = mus.numpy().flatten()\n",
    "        sig = np.exp(logs.numpy()).flatten()\n",
    "        gmm_pdf = sum(pi[k] * norm.pdf(theta_grid, mus[k], sig[k]) for k in range(num_components))\n",
    "        ts = beta.rvs(alpha, beta_param, size=2000)\n",
    "        ss = np.clip(np.hstack([\n",
    "            np.random.normal(mus[k], sig[k], int(round(pi[k] * 2000)))\n",
    "            for k in range(num_components)\n",
    "        ]), 0, 1)\n",
    "        ks, p = ks_2samp(ts, ss)\n",
    "        wd = wasserstein_distance(ts, ss)\n",
    "        ax = plt.subplot(len(test_cases), 1, i + 1)\n",
    "        ax.plot(theta_grid, true_pdf, 'b--')\n",
    "        ax.plot(theta_grid, gmm_pdf, 'r-')\n",
    "        ax.set_title(f\"GMM Unseen n={n}, y={y} | KS={ks:.3f}, p={p:.3f}, W={wd:.3f}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"plots/gmm_unseenN.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "K = 10  \n",
    "hidden_dims = (512, 512, 256, 128)\n",
    "gmm_model = GMMInferenceNet(num_components=K, hidden_dims=hidden_dims)\n",
    "gmm_optimizer = optim.Adam(gmm_model.parameters(), lr=1e-3)\n",
    "print(\"----- Training GMM Network -----\")\n",
    "train_gmm_network(gmm_model, gmm_optimizer)\n",
    "\n",
    "theta_grid = np.linspace(0, 1, 500)\n",
    "y_values_fixed = [1, 100, 2000, 8000]\n",
    "test_cases_unseen = [(20000, 1), (50000, 25000), (200000, 150000)]\n",
    "\n",
    "print(\"----- Evaluating GMM Fixed n=10000 -----\")\n",
    "evaluate_gmm_fixed_n(gmm_model, fixed_n=10000, y_values=y_values_fixed, theta_grid=theta_grid, num_components=K)\n",
    "print(\"----- Evaluating GMM Unseen n's -----\")\n",
    "evaluate_gmm_unseen_n(gmm_model, test_cases_unseen, theta_grid, num_components=K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244171ca-b477-484c-9855-440fa8bcaf2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
